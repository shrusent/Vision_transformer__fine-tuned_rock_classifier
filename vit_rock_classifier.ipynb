{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZGnVo41q1r9z"
      },
      "outputs": [],
      "source": [
        "# pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "2Hy1gJfqIpJ8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy import stats\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from transformers import ViTFeatureExtractor\n",
        "from tabulate import tabulate\n",
        "from transformers import AutoImageProcessor\n",
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow\n",
        "from scipy.stats import pearsonr\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import torch\n",
        "import datasets\n",
        "from tensorflow.keras import layers, models\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import ortho_group\n",
        "from scipy.spatial import procrustes\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "import os\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKT0woYVTFg_"
      },
      "source": [
        "Fine tuning Vision transformer from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x9fVgB0dHAzq"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HLXpD_vRwoPm"
      },
      "outputs": [],
      "source": [
        "def create_image_folder_dataset(root_path):\n",
        "\n",
        "    _CLASS_NAMES= os.listdir(root_path)\n",
        "\n",
        "    features=datasets.Features({\n",
        "                      \"img\": datasets.Image(),\n",
        "                      \"label\": datasets.features.ClassLabel(names=_CLASS_NAMES),\n",
        "                  })\n",
        "\n",
        "    img_data_files=[]\n",
        "    label_data_files=[]\n",
        "\n",
        "    for img_class in os.listdir(root_path):\n",
        "        for img in os.listdir(os.path.join(root_path,img_class)):\n",
        "            path_=os.path.join(root_path,img_class,img)\n",
        "            img_data_files.append(path_)\n",
        "            label_data_files.append(img_class)\n",
        "\n",
        "    ds = datasets.Dataset.from_dict({\"img\":img_data_files,\"label\":label_data_files},features=features)\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ngv9WXjIwsS9"
      },
      "outputs": [],
      "source": [
        "rock_ds = create_image_folder_dataset(\"/content/drive/MyDrive/AML/360 Rocks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "M5nYm6iU85sK"
      },
      "outputs": [],
      "source": [
        "test_ds = create_image_folder_dataset(\"/content/drive/MyDrive/AML/120 Rocks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u97VxbUh89K9",
        "outputId": "deb79f0d-d1ea-41e1-8666-ebd477883f80"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['img', 'label'],\n",
              "    num_rows: 120\n",
              "})"
            ]
          },
          "execution_count": 250,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJnMUmoH7TdO",
        "outputId": "e1a3929d-d3f4-4bcf-d9ca-9d3870cf275d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['img', 'label'],\n",
              "    num_rows: 360\n",
              "})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rock_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "E_Kar5iP7XtG"
      },
      "outputs": [],
      "source": [
        "class_labels = rock_ds.features[\"label\"].names\n",
        "class_labels_test = test_ds.features['label'].names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "624WYAQw7Z66",
        "outputId": "193085d1-20ac-4137-ceac-569721e0421e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['igneous', 'metamorphic', 'sedimentary']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9BChcDC0EXhz",
        "outputId": "5d7abc92-6a65-4144-b068-18a52153ab19"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sedimentary'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "labels = rock_ds.features[\"label\"].names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label\n",
        "\n",
        "id2label[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0cXMv4LlEpPy"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PGYDpHOEhIs",
        "outputId": "2bab0e79-bccf-4864-84d6-5b6a274bbc2c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ViTImageProcessor {\n",
              "  \"do_normalize\": true,\n",
              "  \"do_rescale\": true,\n",
              "  \"do_resize\": true,\n",
              "  \"image_mean\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"image_processor_type\": \"ViTImageProcessor\",\n",
              "  \"image_std\": [\n",
              "    0.5,\n",
              "    0.5,\n",
              "    0.5\n",
              "  ],\n",
              "  \"resample\": 2,\n",
              "  \"rescale_factor\": 0.00392156862745098,\n",
              "  \"size\": {\n",
              "    \"height\": 224,\n",
              "    \"width\": 224\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "image_processor  = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
        "image_processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "16v7DPNCEjSL"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "if \"height\" in image_processor.size:\n",
        "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        "    crop_size = size\n",
        "    max_size = None\n",
        "elif \"shortest_edge\" in image_processor.size:\n",
        "    size = image_processor.size[\"shortest_edge\"]\n",
        "    crop_size = (size, size)\n",
        "    max_size = image_processor.size.get(\"longest_edge\")\n",
        "\n",
        "train_transforms = Compose(\n",
        "        [\n",
        "            RandomResizedCrop(crop_size),\n",
        "            RandomHorizontalFlip(),\n",
        "            ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "val_transforms = Compose(\n",
        "        [\n",
        "            Resize(size),\n",
        "            CenterCrop(crop_size),\n",
        "            ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def preprocess_train(example_batch):\n",
        "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [\n",
        "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"img\"]\n",
        "    ]\n",
        "    return example_batch\n",
        "\n",
        "def preprocess_val(example_batch):\n",
        "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"img\"]]\n",
        "    return example_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fx48y-OmE9Q6",
        "outputId": "b0e5963a-1fdf-4dc3-d6e3-afd1bc1d25ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['img', 'label'],\n",
              "    num_rows: 360\n",
              "})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rock_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-23MOZXZEjPW"
      },
      "outputs": [],
      "source": [
        "# split up training into training and validation\n",
        "splits = rock_ds.train_test_split(test_size=0.333)\n",
        "train_ds = splits['train']\n",
        "val_ds = splits['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE4fAlpXEjNC",
        "outputId": "e57881ff-2ee7-435e-8c3d-43d86f62ecd1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['img', 'label'],\n",
              "    num_rows: 240\n",
              "})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7h_BbDAI17KW",
        "outputId": "551aad9f-75e4-43da-94f7-6e6c07379beb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['img', 'label'],\n",
              "    num_rows: 120\n",
              "})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VV0HH7XcEjKd"
      },
      "outputs": [],
      "source": [
        "train_ds.set_transform(preprocess_train)\n",
        "val_ds.set_transform(preprocess_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "id": "nvunD5uq9WE5"
      },
      "outputs": [],
      "source": [
        "test_ds.set_transform(preprocess_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 295,
      "metadata": {
        "id": "Em_bHG6dIvGH"
      },
      "outputs": [],
      "source": [
        "rock_ds.set_transform(preprocess_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grnagmRSEjIF",
        "outputId": "8ce70b9a-5ab8-445f-ad32-25dd20b2141d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_new = AutoModelForImageClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes = True,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "li85wBfCGCW8"
      },
      "outputs": [],
      "source": [
        "# !pip install accelerate -U\n",
        "# !pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "i8GAAE0vEjFm"
      },
      "outputs": [],
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-vitrock\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SklL1EJIHP9J"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rXem1twbHP6h"
      },
      "outputs": [],
      "source": [
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZN0DTdqzBIwO"
      },
      "outputs": [],
      "source": [
        "model_new.vit.intermediate_layer = nn.Linear(768, 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WPDrAHTZBxTe"
      },
      "outputs": [],
      "source": [
        "model_new.classifier = nn.Linear(768, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mymjFqp8B9is",
        "outputId": "280b2a4c-807c-4537-e3a6-59eed949a0ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ViTForImageClassification(\n",
              "  (vit): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTAttention(\n",
              "            (attention): ViTSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (intermediate_layer): Linear(in_features=768, out_features=8, bias=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4j3pL3OWHP30"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model_new,\n",
        "    args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "Uiun_9oWHP1P",
        "outputId": "e32ae757-5706-4a67-c665-2a4be30fe1d6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 27:01, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.074264</td>\n",
              "      <td>0.458333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.053638</td>\n",
              "      <td>0.483333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.043774</td>\n",
              "      <td>0.525000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_results = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BmfHGmZN6kH"
      },
      "source": [
        "Validation accuracy is 0.52 for this model. Lets save the model and evaluate it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dy6nRwMVHPy4",
        "outputId": "bc137f0f-3b1e-4cf3-e747-831af68273d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               = 51966619GF\n",
            "  train_loss               =     1.0556\n",
            "  train_runtime            = 0:32:06.05\n",
            "  train_samples_per_second =      0.374\n",
            "  train_steps_per_second   =      0.003\n"
          ]
        }
      ],
      "source": [
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "1f0xvoAHHPwT",
        "outputId": "13614194-5373-4d63-dd23-cb608955c0d9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 01:41]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =      0.525\n",
            "  eval_loss               =     1.0438\n",
            "  eval_runtime            = 0:02:31.46\n",
            "  eval_samples_per_second =      0.792\n",
            "  eval_steps_per_second   =      0.026\n"
          ]
        }
      ],
      "source": [
        "metrics = trainer.evaluate()\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BujGGG-jNhaD"
      },
      "source": [
        "Fine tuned the Vision transformer model to output 3 categories and has 8 neurons in the kayer before the last one for performing procrustes analysis.(model_new printed above shows this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p14ty93NyXe"
      },
      "source": [
        "Making a copy of model to extract the weights of the layer before the last one(8 neurons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "So57VEN-zozf"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "model_with_intermediate = copy.deepcopy(model_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {
        "id": "Rv-YCIpXHPjh"
      },
      "outputs": [],
      "source": [
        "intermediate_layer_weights = model_new.vit.intermediate_layer.weight.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekKc8bWdXc37",
        "outputId": "e70e3e6e-bf1b-4d60-8090-0b0eb503e0be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([8, 768])"
            ]
          },
          "execution_count": 233,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "intermediate_layer_weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "z0ziOxsB389b"
      },
      "outputs": [],
      "source": [
        "del model_with_intermediate.vit.intermediate_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "id": "KVTfopkB4T-8"
      },
      "outputs": [],
      "source": [
        "model_with_intermediate.classifier = nn.Linear(in_features=768, out_features=8, bias=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "21E9v35S4ax-"
      },
      "outputs": [],
      "source": [
        "model_with_intermediate.classifier.weight.data = intermediate_layer_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "7D0VSNFK4neK"
      },
      "outputs": [],
      "source": [
        "trainer_new = Trainer(\n",
        "    model_with_intermediate,\n",
        "    args,\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urVxJrF45y6A",
        "outputId": "67e2efc9-532d-4e2c-88ef-7a542dffa46e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0106,  0.0056, -0.0242,  ...,  0.0201, -0.0289, -0.0159],\n",
              "        [-0.0207, -0.0104,  0.0217,  ..., -0.0206, -0.0259, -0.0005],\n",
              "        [-0.0054, -0.0003,  0.0286,  ...,  0.0143, -0.0064,  0.0283],\n",
              "        ...,\n",
              "        [ 0.0237, -0.0086,  0.0283,  ..., -0.0039,  0.0248,  0.0236],\n",
              "        [-0.0074,  0.0313,  0.0097,  ...,  0.0281, -0.0201, -0.0326],\n",
              "        [-0.0333,  0.0196,  0.0177,  ..., -0.0134, -0.0212, -0.0021]])"
            ]
          },
          "execution_count": 228,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model.vit.intermediate_layer.weight.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 310,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv6NvknkOkVQ",
        "outputId": "e282c87f-49d4-4fd0-c03d-08f761bb7a76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0106,  0.0056, -0.0242,  ...,  0.0201, -0.0289, -0.0159],\n",
              "        [-0.0207, -0.0104,  0.0217,  ..., -0.0206, -0.0259, -0.0005],\n",
              "        [-0.0054, -0.0003,  0.0286,  ...,  0.0143, -0.0064,  0.0283],\n",
              "        ...,\n",
              "        [ 0.0237, -0.0086,  0.0283,  ..., -0.0039,  0.0248,  0.0236],\n",
              "        [-0.0074,  0.0313,  0.0097,  ...,  0.0281, -0.0201, -0.0326],\n",
              "        [-0.0333,  0.0196,  0.0177,  ..., -0.0134, -0.0212, -0.0021]])"
            ]
          },
          "execution_count": 310,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_with_intermediate.classifier.weight.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "S0DHs-1q4fiC"
      },
      "outputs": [],
      "source": [
        "# test data- 120 images\n",
        "test_procs = []\n",
        "with torch.no_grad():\n",
        "    for i in range(test_ds.num_rows):\n",
        "      logits = model_with_intermediate(test_ds[i]['pixel_values'].reshape(1,3,224,224)).logits.numpy()\n",
        "      test_procs.append(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "KGR9zDO-F-76"
      },
      "outputs": [],
      "source": [
        "# val data- 120 images split from 360 images\n",
        "val_procs = []\n",
        "with torch.no_grad():\n",
        "    for i in range(val_ds.num_rows):\n",
        "      logits = model_with_intermediate(val_ds[i]['pixel_values'].reshape(1,3,224,224)).logits.numpy()\n",
        "      val_procs.append(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 296,
      "metadata": {
        "id": "RpMy5exDI6wG"
      },
      "outputs": [],
      "source": [
        "# train data + val data - 360 images in total to comapre with human_mds_360 data\n",
        "rock_ds_procs = []\n",
        "with torch.no_grad():\n",
        "    for i in range(rock_ds.num_rows):\n",
        "      logits = model_with_intermediate(rock_ds[i]['pixel_values'].reshape(1,3,224,224)).logits.numpy()\n",
        "      rock_ds_procs.append(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "id": "XYvk0cUqArRQ"
      },
      "outputs": [],
      "source": [
        "test_procs = np.concatenate(test_procs, axis=0)\n",
        "val_procs =np.concatenate(val_procs, axis=0)\n",
        "rock_ds_procs = np.concatenate(rock_ds_procs, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 298,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdd2O9eRBEix",
        "outputId": "54d8a614-60d0-4f7a-ad37-9ff31eaff837"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((120, 8), (120, 8), (360, 8))"
            ]
          },
          "execution_count": 298,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "val_procs.shape, test_procs.shape, rock_ds_procs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "ax2bgyPS4SRj"
      },
      "outputs": [],
      "source": [
        "matrix_with_human_data =np.loadtxt('/content/drive/MyDrive/AML/mds_360.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXQXYJHj4fSv",
        "outputId": "10b40d5e-3662-4c88-d797-a86c49abff75"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(360, 8)"
            ]
          },
          "execution_count": 273,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "matrix_with_human_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "9zJ0wagu46nT"
      },
      "outputs": [],
      "source": [
        "matrix_with_human_data_120 =np.loadtxt('/content/drive/MyDrive/AML/mds_120.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4VVowzJ46eq",
        "outputId": "975ef4df-dac7-4516-92e1-94caa9eefb11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(120, 8)"
            ]
          },
          "execution_count": 275,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "matrix_with_human_data_120.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 299,
      "metadata": {
        "id": "NPtAs5-q4Mik"
      },
      "outputs": [],
      "source": [
        "# procrustes analysis\n",
        "human_data_360, rock_ds, disparity = procrustes(matrix_with_human_data, rock_ds_procs)# for full data 360 images - using 360 mds human data and rock_ds(360 data)\n",
        "human_data_120, val, disparity_val = procrustes(matrix_with_human_data_120, val_procs) # for validation 120 data using 120 human data and 120 val data\n",
        "human_data_120, test, disparity_test = procrustes(matrix_with_human_data_120, test_procs) # for test 120 data 120 human data and 120 test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 301,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXCxd_nu5Bx8",
        "outputId": "b5ee9db6-1bb5-4d95-8111-d36bc4695cec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Disparity for test  data 0.9304034038527171\n",
            "Disparity for validation data 0.9466334812010115\n",
            "Disparity for train data(240) plus validation(120) = (360): 0.9790258189586298\n"
          ]
        }
      ],
      "source": [
        "print(f'Disparity for test  data {disparity_test}')\n",
        "print(f'Disparity for validation data {disparity_val}')\n",
        "print(f'Disparity for train data(240) plus validation(120) = (360): {disparity}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 302,
      "metadata": {
        "id": "lNiY5Ifa53R4"
      },
      "outputs": [],
      "source": [
        "def compute_corr(mtx1, mtx2):\n",
        "    correlations = []\n",
        "    for i in range(mtx1.shape[1]):\n",
        "        correlation, _ = pearsonr(mtx1[:, i], mtx2[:, i])\n",
        "        correlations.append(correlation)\n",
        "    return correlations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 303,
      "metadata": {
        "id": "QPQZ_zm45P6u"
      },
      "outputs": [],
      "source": [
        "correlations_test = compute_corr(human_data_120, test)\n",
        "correlations_val = compute_corr(human_data_120, val)\n",
        "correlations_360 = compute_corr(human_data_360, rock_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFauy-ZPMFsD",
        "outputId": "e53c8035-bb8e-4f8f-a872-9788ceaa8471"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pearson correlation between each dimension of  validation data, test data with the human data-120_mds and the avg of test and val correlations.\n",
            "Correlation between 8 features of human data (360_mds) and 360 images(whole data)\n",
            "+--------------+-------------------+-------------+---------------------+-----------------------+\n",
            "| 8 Features   |   Validation data |   Test data |   Avg_corr_val_test |   360_rocks_train_val |\n",
            "+==============+===================+=============+=====================+=======================+\n",
            "| Feature 1    |          0.17365  |    0.241073 |            0.207361 |             0.161476  |\n",
            "+--------------+-------------------+-------------+---------------------+-----------------------+\n",
            "| Feature 2    |          0.268372 |    0.2488   |            0.258586 |             0.228906  |\n",
            "+--------------+-------------------+-------------+---------------------+-----------------------+\n",
            "| Feature 3    |          0.238778 |    0.2223   |            0.230539 |             0.0927323 |\n",
            "+--------------+-------------------+-------------+---------------------+-----------------------+\n",
            "| Feature 4    |          0.272701 |    0.19314  |            0.23292  |             0.129887  |\n",
            "+--------------+-------------------+-------------+---------------------+-----------------------+\n",
            "| Feature 5    |          0.274275 |    0.464608 |            0.369441 |             0.093889  |\n",
            "+--------------+-------------------+-------------+---------------------+-----------------------+\n",
            "| Feature 6    |          0.1437   |    0.223829 |            0.183765 |             0.131039  |\n",
            "+--------------+-------------------+-------------+---------------------+-----------------------+\n",
            "| Feature 7    |          0.25256  |    0.312721 |            0.282641 |             0.158959  |\n",
            "+--------------+-------------------+-------------+---------------------+-----------------------+\n",
            "| Feature 8    |          0.249676 |    0.224785 |            0.23723  |             0.133845  |\n",
            "+--------------+-------------------+-------------+---------------------+-----------------------+\n",
            "| Mean         |          0.234214 |    0.266407 |            0.25031  |             0.141342  |\n",
            "+--------------+-------------------+-------------+---------------------+-----------------------+\n"
          ]
        }
      ],
      "source": [
        "header = [\"8 Features\", 'Validation data', 'Test data', 'Avg_corr_val_test', '360_rocks_train_val']\n",
        "table_data = []\n",
        "\n",
        "for i in range(len(correlations_test)):\n",
        "    avg_corr_val_test = (correlations_val[i] + correlations_test[i]) / 2\n",
        "    row = [f\"Feature {i + 1}\", correlations_val[i], correlations_test[i], avg_corr_val_test, correlations_360[i]]\n",
        "    table_data.append(row)\n",
        "\n",
        "mean_row = ['Mean'] + [np.mean([row[j] for row in table_data]) for j in range(1, len(header))]\n",
        "table_data.append(mean_row)\n",
        "\n",
        "table = tabulate(table_data, headers=header, tablefmt=\"grid\")\n",
        "print(\"Pearson correlation between each dimension of  validation data, test data with the human data-120_mds and the avg of test and val correlations.\")\n",
        "print('Correlation between 8 features of human data (360_mds) and 360 images(whole data)')\n",
        "print(table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A26hOH4Oqvz"
      },
      "source": [
        "Computed the procrustes analysis for both test data(120 images), validation data(120 images which was split from 360 images), with the human_data 120_mds.txt separetely and reporting the disparities above.\n",
        "\n",
        "Also computed the procustes analysis for whole 360 images rock_ds data with the human data (360_mds.txt) for comparison.\n",
        "\n",
        "As the question states to report the correlation values for test and validation datasets, I have reported them in the table along with 360 images(train+val) data correlations as well.\n",
        "Also computed the average correlation between test and validation data in a separete column Avg_corr_val_test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b90-HfQRGW7"
      },
      "source": [
        " **The average correlation coefficient for test and validation data with human_120_mds data for 8 features is\n",
        "0.207361,\n",
        "0.258586,\n",
        "0.230539,\n",
        "0.23292,\n",
        "0.369441,\n",
        "0.183765,\n",
        "0.282641,\n",
        "0.23723**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4mnLSaURqir"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in-sUjFRTRh_"
      },
      "source": [
        "# References:\n",
        "\n",
        "\n",
        "\n",
        "1.   https://huggingface.co/google/vit-base-patch16-224-in21k\n",
        "2.   https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer\n",
        "3. https://huggingface.co/docs/transformers/main_classes/trainer\n",
        "4. https://github.com/huggingface/notebooks/blob/main/examples/image_classification.ipynb\n",
        "5. https://github.com/rajshah4/huggingface-demos/blob/main/FoodApp/Indian_food_image_classification_fine-tuning.ipynb\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
